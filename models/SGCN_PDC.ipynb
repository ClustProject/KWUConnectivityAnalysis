{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0893427",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "import numpy as np\n",
    "from torch_geometric.nn import ChebConv, GCNConv\n",
    "from torch_geometric.transforms import LaplacianLambdaMax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa900f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EEG_band : delta, theta, alpha, beta, gamma, all = 1,2,3,4,5,None\n",
    "# Feature_name : de_LDS, PSD_LDS, etc.\n",
    "def load_data(data_dir_path,label_dir_path, trial:int, feature_name, EEG_band=None):  \n",
    "    subject_data_list, session_list = [], []\n",
    "    file_list = os.listdir(data_dir_path)\n",
    "    \n",
    "    for idx, file in enumerate(file_list):\n",
    "        trial_list = []\n",
    "        data = io.loadmat(data_dir_path + file)\n",
    "        if EEG_band is None:\n",
    "            for trial_idx in range(1, trial+1):\n",
    "                trial_list.append(data[feature_name + str(trial_idx)][:, :, :])  # EEG all bands\n",
    "\n",
    "        else:\n",
    "            for trial_idx in range(1, trial+1):\n",
    "                trial_list.append(data[feature_name + str(trial_idx)][:, :, EEG_band])\n",
    "\n",
    "        session_list.append(trial_list)\n",
    "\n",
    "        if (idx + 1) % 3 == 0:\n",
    "            subject_data_list.append(session_list)\n",
    "            session_list = []\n",
    "\n",
    "    label = io.loadmat(label_dir_path + 'label.mat')['label']\n",
    "    label = [1 + label[0][i] for i in range(trial)]\n",
    "    #label [-1, 1] --> [0, 2]\n",
    "\n",
    "    # subject_data_list : num_subjects(15) × num_sessions(3) × num_trials(15)\n",
    "    return subject_data_list, label\n",
    "\n",
    "\n",
    "# edge attributes are composed of edge weights, the distance between all EEG channel pairs\n",
    "def load_edge_information(pdc_dir_path, sub_name, pdc_var_name, n_channels, n_trials, n_sessions, n_subjects):\n",
    "\n",
    "    file_list = os.listdir(pdc_dir_path)\n",
    "    \n",
    "    edge_index_list = []\n",
    "    for i in range(n_channels):\n",
    "        for j in range(n_channels):\n",
    "            edge_index_list.append([i,j])\n",
    "            \n",
    "    edge_attr_list = []\n",
    "    session_edge_attr_list = []\n",
    "    sub_idx = 0\n",
    "    for i, file in enumerate(file_list):\n",
    "        trial_edge_attr_list = []\n",
    "        pdcs = io.loadmat(pdc_dir_path + file)\n",
    "        pdc_name = sub_name[sub_idx]+pdc_var_name\n",
    "        for trial_idx in range(1, n_trials+1):\n",
    "            edge_attr = []\n",
    "            pdc = pdcs[pdc_name+str(trial_idx)][:,:]\n",
    "            for k in range(n_channels):\n",
    "                for l in range(n_channels):\n",
    "                    if k == l:\n",
    "                        edge_attr.append(0)\n",
    "                    else:\n",
    "                        edge_attr.append(pdc[k][l])\n",
    "        \n",
    "            trial_edge_attr_list.append(edge_attr)\n",
    "        session_edge_attr_list.append(trial_edge_attr_list)\n",
    "        if (i+1) % 3 == 0:\n",
    "            sub_idx+=1\n",
    "            edge_attr_list.append(session_edge_attr_list)\n",
    "            session_edge_attr_list = []\n",
    "\n",
    "    return edge_index_list, edge_attr_list\n",
    "\n",
    "\n",
    "#Graph Representation\n",
    "def get_graph_data(subject_data, subject_label, edge_index_list, edge_attr_list, num_train_trials , batch_size):\n",
    "    edge_index = torch.tensor(edge_index_list, dtype=torch.long)\n",
    "    train_loader, test_loader = [], []\n",
    "\n",
    "    num_subjects = len(subject_data)\n",
    "    num_sessions = len(subject_data[0])\n",
    "    num_trials = len(subject_data[0][0])\n",
    "\n",
    "    for subject in range(num_subjects):\n",
    "        train_dataset, test_dataset = [], []\n",
    "        for session in range(num_sessions):\n",
    "            for trial in range(num_trials):\n",
    "                data_list = []\n",
    "                trial_data = subject_data[subject][session][trial] # trial_data = [62][about 240][1or5] = [nodes][trial time][EEG band(s)]\n",
    "                blocks = len(trial_data[1]) # about 240(240 sec, 4minutes), 'blocks' is number of blocks which is the same as a trial time\n",
    "                edge_attr = torch.tensor(edge_attr_list[subject][session][trial], dtype=torch.float)\n",
    "                for block_idx in range(blocks):\n",
    "                    # if using features of all frequency bands,\n",
    "                    # node_feature = [delta, theta, alpha, beta, gamma]\n",
    "                    data_sample = torch.tensor(trial_data[:, block_idx, :], dtype=torch.float)#data_sample = [62][5] = [nodes, node_features(EEG band(s)]\n",
    "                    data_label = torch.tensor(subject_label[trial], dtype=torch.long) # data_label [0,2]\n",
    "                    data_list.append(Data(x=data_sample, edge_index=edge_index.t().contiguous(), edge_attr=edge_attr, y=data_label))\n",
    "                if trial < num_train_trials: \n",
    "                    train_dataset.extend(data_list)\n",
    "                else: \n",
    "                    test_dataset.extend(data_list)\n",
    "            \n",
    "        random.shuffle(train_dataset)\n",
    "        random.shuffle(test_dataset)\n",
    "        \n",
    "        batch_train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "        batch_test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "        train_loader.append(batch_train_loader)\n",
    "        test_loader.append(batch_test_loader)\n",
    "        print('loading' + str(subject))\n",
    "    print(\"\\nTrain dataset length: {}, \\tTest dataset legnth: {}\".format(len(train_dataset), len(test_dataset)))\n",
    "    return train_loader, test_loader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346e9cd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def criterion(loss, label, model, L1_regularization_scaler):\n",
    "    l1_regularization = torch.tensor(0., device=device)\n",
    "    \n",
    "    for param in model.parameters():\n",
    "        l1_regularization += torch.norm(param, 1)\n",
    "\n",
    "    loss += L1_regularization_scaler * l1_regularization\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train(loader, model, optimizer, L1_regularization_scaler, epoch, batch_size):\n",
    "    model.train()\n",
    "    train_acc, train_loss, count = 0., 0., 0.\n",
    "\n",
    "    dataset_length = len(loader.dataset)\n",
    "    loader_length = len(loader)\n",
    "    for batch_idx, data in enumerate(loader):\n",
    "        if len(data.y) == batch_size: # dataset size가 batch size로 안나눠지면 버림\n",
    "            count += 1.\n",
    "            data = data.to(device)\n",
    "            loss,result = model(data, 'train')\n",
    "            loss = criterion(loss, data.y, model, L1_regularization_scaler)\n",
    "            loss.backward()\n",
    "\n",
    "#             with torch.no_grad():\n",
    "#                 for p in model.parameters():\n",
    "#                     #p = (1-lr)*p + lr*p.grad\n",
    "#                     print(p.requires_grad,\" row : \", p)\n",
    "#                     p = p.grad\n",
    "#                     print(p.requires_grad, \" copy : \",p)\n",
    "#                     break\n",
    "#             for p in model.parameters():\n",
    "#                 print(p.requires_grad, \" update: \", p)\n",
    "#                 break\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            train_loss += loss.item()\n",
    "            pred = result.max(1, keepdim = True)[1]\n",
    "            acc = pred.eq(data.y.view_as(pred)).sum().item()\n",
    "            acc = 100. * acc / batch_size\n",
    "\n",
    "            train_acc += acc\n",
    "\n",
    "    train_acc /= count\n",
    "    train_loss /= count\n",
    "    return train_acc, train_loss\n",
    "\n",
    "\n",
    "def test(loader, model, L1_regularization_scaler, batch_size):\n",
    "    model.eval()\n",
    "    count, test_loss, test_acc = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            if len(data.y) == batch_size:\n",
    "                count += 1\n",
    "                data = data.to(device)\n",
    "                output, result = model(data)\n",
    "                loss = output.item()\n",
    "                test_loss += loss\n",
    "                \n",
    "                pred = result.max(1, keepdim = True)[1]\n",
    "                acc = pred.eq(data.y.view_as(pred)).sum().item()\n",
    "                acc = 100. * acc / batch_size\n",
    "                test_acc += acc\n",
    "\n",
    "    test_acc /= count\n",
    "    test_loss /= count\n",
    "    return test_acc, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b9d4a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SSLGCN(nn.Module):\n",
    "    def __init__(self, num_nodes,num_features, gcn1_channels, gcn2_channels, gcn3_channels, fc1_channels, out_channels, edge_weight, batch_size, learnable=False):\n",
    "        super(SSLGCN, self).__init__()\n",
    "        self.in_channels = num_features\n",
    "        self.gcn1_out_channels = gcn1_channels\n",
    "        self.gcn2_out_channels = gcn2_channels\n",
    "        self.gcn3_out_channels = gcn3_channels\n",
    "#         self.fc1_in_channels = (gcn1_channels + gcn2_channels) * num_nodes\n",
    "        self.fc1_in_channels = gcn3_channels * num_nodes\n",
    "        self.fc1_out_channels = fc1_channels\n",
    "        self.out_channels = out_channels\n",
    "        \n",
    "        self.lambdamax = LaplacianLambdaMax(None)\n",
    "        self.edge_weight = nn.Parameter(edge_weight, requires_grad=learnable)\n",
    "        self.batch_size = batch_size\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.crossentropy = nn.CrossEntropyLoss()\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leakyrelu = nn.LeakyReLU(0.15)\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        \n",
    "        self.bn1d = nn.BatchNorm1d(self.in_channels)\n",
    "        self.gcn1 = GCNConv(self.in_channels, self.gcn1_out_channels)\n",
    "        self.gcn2 = GCNConv(self.gcn1_out_channels, self.gcn2_out_channels)\n",
    "        self.gcn3 = GCNConv(self.gcn2_out_channels, self.gcn3_out_channels)\n",
    "        self.fc1 = nn.Linear(self.fc1_in_channels, self.fc1_out_channels)\n",
    "        self.fc2 = nn.Linear(self.fc1_out_channels, self.out_channels)\n",
    "        self.fc_module = nn.Sequential(self.fc1, self.dropout, self.leakyrelu, self.fc2)\n",
    "        \n",
    "    def forward(self, data, _type=None):\n",
    "        data.edge_attr = self.edge_weight.data.repeat(self.batch_size)\n",
    "        data = self.lambdamax(data)\n",
    "\n",
    "        if data.x.dim() == 1:\n",
    "            data.x = data.x.unsqueeze(dim=1)\n",
    "        \n",
    "        bn = self.leakyrelu(self.bn1d(data.x))\n",
    "        gcn1 = self.gcn1(bn, data.edge_index, self.leakyrelu(data.edge_attr))\n",
    "        gcn2 = self.gcn2(self.leakyrelu(gcn1), data.edge_index, self.leakyrelu(data.edge_attr))\n",
    "        gcn3 = self.gcn3(self.leakyrelu(gcn2), data.edge_index, self.leakyrelu(data.edge_attr))\n",
    "        \n",
    "#         gcn = torch.cat((gcn1, gcn2), 0)\n",
    "        gcn = self.leakyrelu(gcn3).reshape(self.batch_size, -1)\n",
    "\n",
    "        fc = self.fc_module(gcn)\n",
    "\n",
    "        loss = self.crossentropy(fc.view(-1, self.out_channels), data.y.view(-1))\n",
    "        result = self.softmax(fc)\n",
    "        \n",
    "        return loss, result\n",
    "\n",
    "class DGCNN(nn.Module): # Two GCN with Batch Normalization and LeakyReLU\n",
    "    def __init__(self, num_nodes,num_features, hid_channels, out_channels, k,  edge_weight, batch_size, learnable=False):\n",
    "        super(DGCNN, self).__init__()\n",
    "\n",
    "        self.lambdamax = LaplacianLambdaMax(None)\n",
    "\n",
    "        self.in_channels = num_features\n",
    "        self.cheb_out_channels = hid_channels\n",
    "\n",
    "        self.fc1_in_channels = 1240\n",
    "        self.fc1_out_channels = 128 # \n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        self.edge_weight = nn.Parameter(edge_weight, requires_grad=learnable)\n",
    "        self.batch_size = batch_size\n",
    "        self.num_nodes = num_nodes\n",
    "        \n",
    "        self.chebconv1 = ChebConv(self.in_channels, self.cheb_out_channels, K=k, normalization = None)\n",
    "        # batch normalization\n",
    "        \n",
    "        self.BN1d1 = nn.BatchNorm1d(self.in_channels)\n",
    "        #self.BN1d1 = nn.BatchNorm1d(self.cheb_out_channels)\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.leakyrelu = nn.LeakyReLU(0.15)\n",
    "        \n",
    "        self.fc1 = nn.Linear(self.fc1_in_channels, self.fc1_out_channels)\n",
    "        self.fc2 = nn.Linear(self.fc1_out_channels, self.out_channels)\n",
    "        self.fc_module = nn.Sequential(self.fc1, self.leakyrelu, self.fc2)\n",
    "\n",
    "        \n",
    "                                      \n",
    "    def forward(self, data, _type=None):\n",
    "        data.edge_attr = self.edge_weight.data.repeat(self.batch_size)\n",
    "        data = self.lambdamax(data)\n",
    "\n",
    "        if data.x.dim() == 1:\n",
    "            data.x = data.x.unsqueeze(dim=1)\n",
    "\n",
    "        data.x = self.leakyrelu(self.BN1d1(data.x))\n",
    "\n",
    "        cheb_layer = self.chebconv1(data.x, data.edge_index, self.leakyrelu(data.edge_attr), lambda_max=data.lambda_max)\n",
    "\n",
    "        cheb_layer = self.leakyrelu(cheb_layer).reshape(self.batch_size, -1)\n",
    "        fc_layer = self.fc_module(cheb_layer)\n",
    "        \n",
    "        if _type == 'train':\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(fc_layer.view(-1, self.out_channels), data.y.view(-1))\n",
    "            logits = self.softmax(fc_layer)\n",
    "            return loss, logits\n",
    "        else:\n",
    "            loss_fct = nn.CrossEntropyLoss()\n",
    "            loss = loss_fct(fc_layer.view(-1, self.out_channels), data.y.view(-1))\n",
    "            logits = self.softmax(fc_layer)\n",
    "            return loss, logits\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c16eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir_path = 'D:/fragmentation project/Emotion_Recognition/DGCNN/GCN_implement/GCN_implement/dataset/SEED/SEED/ExtractedFeatures/data/'\n",
    "feature_name = 'de_LDS'\n",
    "# data_dir_path = 'C:/Users/daehyeon/Desktop/GCN_implement/GCN_implement/dataset/SEED/SEED/ExtractedFeatures/data/'\n",
    "# feature_name = 'de_LDS'\n",
    "label_dir_path = 'D:/fragmentation project/Emotion_Recognition/DGCNN/GCN_implement/GCN_implement/dataset/SEED/SEED/ExtractedFeatures/label/'\n",
    "channels_dir_path = 'D:/fragmentation project/Emotion_Recognition/DGCNN/GCN_implement/GCN_implement/requirements'\n",
    "\n",
    "pdc_dir_path = 'E:/asympPDC-main/pdcs_nodiag/'\n",
    "sub_name = ['djc', 'jl', 'jj', 'lqj', 'ly', 'mhw', 'phl', 'sxy', 'wk', 'ww', 'wsf', 'wyw', 'xyl', 'ys', 'zjy'];\n",
    "pdc_var_name = '_PDC_mean'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e0e75f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions, subjects, trials, num_nodes, num_classes, train_trials = 3, 15, 15, 62, 3, 9  # 논문과 데이터셋에서 명시된 값\n",
    "epochs, freq_bands, random_seed = 100, 5, 42  # training 할 때 변경이 가능한 hyper-parameter\n",
    "k, batch_size, learning_rate, lambda1 = 3, 32, 0.01, 0.001  # training 할 때 변경이 가능한 parameter\n",
    "EEG_band = None\n",
    "subject_data, subject_label = load_data(data_dir_path,label_dir_path, trials, feature_name, EEG_band)\n",
    "edge_index_list, edge_attr_list = load_edge_information(pdc_dir_path, sub_name, pdc_var_name, num_nodes, trials, sessions, subjects)\n",
    "train_loader, test_loader = get_graph_data(subject_data, subject_label,\n",
    "                                 edge_index_list, edge_attr_list,\n",
    "                                 train_trials , batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1516c133",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_train_loader_pdc = train_loader[4]\n",
    "sub_edge_attr_pdc = sub_train_loader_pdc.dataset[4].edge_attr\n",
    "sub_test_loader_pdc = test_loader[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f652b9b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0124168",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 200\n",
    "\n",
    "# prev = 0\n",
    "# best_acc = np.zeros((15), float)\n",
    "\n",
    "for sub in range (10,11):\n",
    "    cnt = 0\n",
    "    learning_rate = 0.0001\n",
    "\n",
    "    sub_train_loader_pdc = train_loader[sub]\n",
    "    sub_edge_attr_pdc = sub_train_loader_pdc.dataset[sub].edge_attr\n",
    "    sub_test_loader_pdc = test_loader[sub]\n",
    "    len_lr = 3;\n",
    "    len_ld = 5;\n",
    "    \n",
    "    f = open('E:/pdc_result/all_sub_dgcnn/continuesss_22-08-31 sub' + str(sub+1) + '_DGCNN_PDC_epoch200.txt', 'w')\n",
    "    \n",
    "    for lr in range (len_lr):\n",
    "        lambda1 = 0.001\n",
    "        \n",
    "        for ld in range(len_ld):\n",
    "    #         learning_check = 0\n",
    "            cnt+=1\n",
    "            print(\"OK {}\".format(cnt))\n",
    "            w = \"lr: \"+str(learning_rate) + \"  ld: \"+str(lambda1) + \"\\n--------------------------------------------------------------------\\n\"\n",
    "            f.write(w)\n",
    "            model6 = DGCNN6(num_nodes, freq_bands, 20, num_classes, k, sub_edge_attr_pdc, batch_size, True).to(device)\n",
    "            optimizer6 = torch.optim.Adam(model6.parameters(), lr=learning_rate)#, weight_decay=5e-3)\n",
    "            for epoch in range(1, epochs+1):\n",
    "                train_acc, train_loss = train(sub_train_loader_pdc, model6, optimizer6, lambda1, epoch, batch_size)\n",
    "\n",
    "                if epoch % 10 == 0:\n",
    "                    a = \"Epoch: \"+str(epoch)+\", Train Acc: \" + str(round(train_acc,2)) + \",Train Loss: \" + str(round(train_loss,4)) + \"\\n\"\n",
    "                    f.write(a)\n",
    "                test_acc, test_loss = test(sub_test_loader_pdc, model6, lambda1, batch_size)\n",
    "                if test_acc > best_acc[sub]:\n",
    "                    best_acc[sub] = test_acc\n",
    "                b = \"Epoch: \"+str(epoch)+\", Model6 Evaluation Result -  Test Accuracy: \" + str(round(test_acc,2)) + \",\\tTest Loss: \" + str(round(test_loss,4)) + \"\\n\\n\"\n",
    "                f.write(b)\n",
    "\n",
    "    #             if str(test_acc)[:5] == str(prev)[:5]:\n",
    "    #                 learning_check += 1\n",
    "    #             else:\n",
    "    #                 learning_check = 0\n",
    "    #             if learning_check == 8:\n",
    "    #                 break\n",
    "\n",
    "    #             prev = test_acc\n",
    "            lambda1 /= 10\n",
    "            f.write(\"------------------------------------------------------------------\\n\\n\")\n",
    "        f.write(\"------------------------------------------------------------------\\n\\n\\n\")\n",
    "        learning_rate /= 10\n",
    "    f.write(\"!!! BEST ACCURACY !!!\\n\")\n",
    "    f.write(str(best_acc[sub]))\n",
    "    f.write(\"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "print(\"\\n-------- Best Acc ---------------\")\n",
    "print(best_acc)\n",
    "\n",
    "f = open('E:/pdc_result/all_sub_dgcnn/22-08-31 sub_acc_mean_std', 'w')\n",
    "mean_acc = best_acc.mean()\n",
    "std_acc = best_acc.std()\n",
    "f.write(\"mean_acc : \")\n",
    "f.write(str(mean_acc))\n",
    "f.write(\"std_acc: \")\n",
    "f.write(str(std_acc))\n",
    "f.close()\n",
    "\n",
    "print(\"\\n-------------------------------\")\n",
    "print(\"mean_acc : \", mean_acc)\n",
    "print(\"std_acc: \", std_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d18fe9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
